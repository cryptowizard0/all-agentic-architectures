{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro-title",
   "metadata": {},
   "source": [
    "# ğŸ“˜ Agentic æ¶æ„ 11ï¼šå…ƒæ§åˆ¶å™¨ (Meta-Controller)\n",
    "\n",
    "æ¬¢è¿æ¥åˆ°æˆ‘ä»¬ç³»åˆ—çš„ç¬¬åä¸€ä¸ªç¬”è®°æœ¬ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ„å»ºä¸€ä¸ª**å…ƒæ§åˆ¶å™¨**ï¼ˆMeta-Controllerï¼‰ï¼Œè¿™æ˜¯ä¸€ç§ç›‘ç£å¼ä»£ç†æ¶æ„ï¼Œç”¨äºåè°ƒä¸€ç»„ä¸“é•¿å­ä»£ç†ã€‚è¿™ç§æ¨¡å¼æ˜¯åˆ›å»ºå¼ºå¤§ã€å¤šæ‰å¤šè‰º AI ç³»ç»Ÿçš„åŸºçŸ³ã€‚\n",
    "\n",
    "ä¸å…¶æ„å»ºä¸€ä¸ªè¯•å›¾åŒ…åŠä¸€åˆ‡çš„å•ä¸€ã€åºå¤§ä»£ç†ï¼Œä¸å¦‚è®©å…ƒæ§åˆ¶å™¨å……å½“æ™ºèƒ½è°ƒåº¦å™¨ã€‚å®ƒæ¥æ”¶ä¼ å…¥è¯·æ±‚ï¼Œåˆ†æè¯·æ±‚çš„æ€§è´¨ï¼Œå¹¶å°†å…¶è·¯ç”±åˆ°å¯ç”¨ä»£ç†æ± ä¸­æœ€åˆé€‚çš„ä¸“é•¿ä»£ç†ã€‚è¿™ä½¿å¾—æ¯ä¸ªå­ä»£ç†éƒ½èƒ½é’ˆå¯¹å…¶ç‰¹å®šä»»åŠ¡è¿›è¡Œé«˜åº¦ä¼˜åŒ–ï¼Œä»è€Œæå‡æ•´ä½“æ€§èƒ½å’Œæ¨¡å—åŒ–ç¨‹åº¦ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†é€šè¿‡æ„å»ºä¸€ä¸ªåŒ…å«ä¸‰ä¸ªä¸åŒä¸“é•¿ä»£ç†çš„ç³»ç»Ÿæ¥æ¼”ç¤ºè¿™ä¸€æ¶æ„ï¼š\n",
    "1.  **é€šç”¨ä»£ç†ï¼š** å¤„ç†æ—¥å¸¸å¯¹è¯å’Œç®€å•é—®é¢˜ã€‚\n",
    "2.  **ç ”ç©¶ä»£ç†ï¼š** é…å¤‡æœç´¢å·¥å…·ï¼Œç”¨äºå›ç­”å…³äºè¿‘æœŸäº‹ä»¶æˆ–å¤æ‚ä¸»é¢˜çš„é—®é¢˜ã€‚\n",
    "3.  **ç¼–ç ä»£ç†ï¼š** ä¸“æ³¨äºç”Ÿæˆ Python ä»£ç ç‰‡æ®µçš„ä¸“é•¿ä»£ç†ã€‚\n",
    "\n",
    "å…ƒæ§åˆ¶å™¨å°†æˆä¸ºæ•´ä¸ªç³»ç»Ÿçš„â€œå¤§è„‘â€ï¼Œå®ƒä¼šæ£€æŸ¥æ¯ä¸ªç”¨æˆ·æŸ¥è¯¢ï¼Œå¹¶å†³å®šå“ªä¸ªä»£ç†æœ€é€‚åˆå“åº”ã€‚è¿™åˆ›é€ äº†ä¸€ä¸ªçµæ´»ä¸”æ˜“äºæ‰©å±•çš„ç³»ç»Ÿï¼Œåªéœ€åˆ›å»ºæ–°çš„ä¸“é•¿ä»£ç†å¹¶è®©æ§åˆ¶å™¨äº†è§£å®ƒï¼Œå³å¯æ·»åŠ æ–°åŠŸèƒ½ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intro-definition",
   "metadata": {},
   "source": [
    "### å®šä¹‰\n",
    "**å…ƒæ§åˆ¶å™¨**ï¼ˆMeta-Controllerï¼Œæˆ–ç§°è·¯ç”±å™¨ï¼‰æ˜¯ä¸€ç§å¤šä»£ç†ç³»ç»Ÿä¸­çš„ç›‘ç£ä»£ç†ï¼Œè´Ÿè´£åˆ†æä¼ å…¥ä»»åŠ¡å¹¶å°†å…¶åˆ†æ´¾ç»™åˆé€‚çš„ä¸“é•¿å­ä»£ç†æˆ–å·¥ä½œæµç¨‹ã€‚å®ƒå……å½“æ™ºèƒ½è·¯ç”±å±‚ï¼Œå†³å®šå“ªä¸ªå·¥å…·æˆ–ä¸“å®¶æœ€é€‚åˆå¤„ç†å½“å‰ä»»åŠ¡ã€‚\n",
    "\n",
    "### é«˜å±‚å·¥ä½œæµç¨‹\n",
    "\n",
    "1.  **æ¥æ”¶è¾“å…¥ï¼š** ç³»ç»Ÿæ¥æ”¶ç”¨æˆ·è¯·æ±‚ã€‚\n",
    "2.  **å…ƒæ§åˆ¶å™¨åˆ†æï¼š** å…ƒæ§åˆ¶å™¨ä»£ç†æ£€æŸ¥è¯·æ±‚çš„æ„å›¾ã€å¤æ‚åº¦å’Œå†…å®¹ã€‚\n",
    "3.  **åˆ†æ´¾ç»™ä¸“é•¿ä»£ç†ï¼š** æ ¹æ®åˆ†æç»“æœï¼Œå…ƒæ§åˆ¶å™¨ä»é¢„å®šä¹‰çš„ä»£ç†æ± ä¸­é€‰æ‹©æœ€åˆé€‚çš„ä¸“é•¿ä»£ç†ï¼ˆä¾‹å¦‚ï¼Œâ€œç ”ç©¶è€…â€ã€â€œç¼–ç è€…â€ã€â€œèŠå¤©æœºå™¨äººâ€ï¼‰ã€‚\n",
    "4.  **æ‰§è¡Œä»»åŠ¡ï¼š** è¢«é€‰ä¸­çš„ä¸“é•¿ä»£ç†æ‰§è¡Œä»»åŠ¡å¹¶ç”Ÿæˆç»“æœã€‚\n",
    "5.  **è¿”å›ç»“æœï¼š** ä¸“é•¿ä»£ç†çš„ç»“æœè¿”å›ç»™ç”¨æˆ·ã€‚åœ¨æ›´å¤æ‚çš„å·¥ä½œæµç¨‹ä¸­ï¼Œæ§åˆ¶æƒå¯èƒ½è¿”å›åˆ°å…ƒæ§åˆ¶å™¨ä»¥è¿›è¡Œåç»­æ­¥éª¤æˆ–ç›‘æ§ã€‚\n",
    "\n",
    "### ä½•æ—¶ä½¿ç”¨ / åº”ç”¨åœºæ™¯\n",
    "*   **å¤šæœåŠ¡ AI å¹³å°ï¼š** ä¸ºæä¾›å¤šç§æœåŠ¡çš„å¹³å°ï¼ˆå¦‚æ–‡æ¡£åˆ†æã€æ•°æ®å¯è§†åŒ–ã€åˆ›æ„å†™ä½œï¼‰æä¾›å•ä¸€å…¥å£ã€‚\n",
    "*   **è‡ªé€‚åº”ä¸ªäººåŠ©ç†ï¼š** èƒ½å¤Ÿåœ¨ä¸åŒæ¨¡å¼æˆ–å·¥å…·ä¹‹é—´åˆ‡æ¢çš„åŠ©ç†ï¼Œä¾‹å¦‚ç®¡ç†æ—¥å†ã€æœç´¢ç½‘ç»œæˆ–æ§åˆ¶æ™ºèƒ½å®¶å±…è®¾å¤‡ã€‚\n",
    "*   **ä¼ä¸šå·¥ä½œæµç¨‹ï¼š** æ ¹æ®å·¥å•å†…å®¹å°†å®¢æˆ·æ”¯æŒå·¥å•è·¯ç”±åˆ°æ­£ç¡®çš„éƒ¨é—¨ï¼ˆæŠ€æœ¯ã€è®¡è´¹ã€é”€å”®ï¼‰ã€‚\n",
    "\n",
    "### ä¼˜åŠ¿ä¸åŠ£åŠ¿\n",
    "*   **ä¼˜åŠ¿ï¼š**\n",
    "    *   **çµæ´»æ€§ä¸æ¨¡å—åŒ–ï¼š** é€šè¿‡ç®€å•æ·»åŠ æ–°çš„ä¸“é•¿ä»£ç†å¹¶æ›´æ–°æ§åˆ¶å™¨çš„è·¯ç”±é€»è¾‘ï¼Œå³å¯ææ˜“æ‰©å±•æ–°åŠŸèƒ½ã€‚\n",
    "    *   **æ€§èƒ½ï¼š** å…è®¸ä½¿ç”¨é«˜åº¦ä¼˜åŒ–çš„ä¸“å®¶ä»£ç†ï¼Œè€Œä¸æ˜¯ä¸€ä¸ªæ ·æ ·é€šä½†æ ·æ ·æ¾çš„ä¸‡é‡‘æ²¹æ¨¡å‹ã€‚\n",
    "*   **åŠ£åŠ¿ï¼š**\n",
    "    *   **æ§åˆ¶å™¨æˆä¸ºå•ç‚¹æ•…éšœï¼š** æ•´ä¸ªç³»ç»Ÿçš„è´¨é‡é«˜åº¦ä¾èµ–æ§åˆ¶å™¨æ­£ç¡®è·¯ç”±ä»»åŠ¡çš„èƒ½åŠ›ã€‚ä¸€æ¬¡é”™è¯¯çš„è·¯ç”±å†³ç­–ä¼šå¯¼è‡´æ¬¡ä¼˜æˆ–é”™è¯¯çš„ç»“æœã€‚\n",
    "    *   **æ½œåœ¨å»¶è¿Ÿå¢åŠ ï¼š** ä¸ç›´æ¥è°ƒç”¨å•ä¸€ä»£ç†ç›¸æ¯”ï¼Œé¢å¤–çš„è·¯ç”±æ­¥éª¤å¯èƒ½ä¼šå¢åŠ å°‘é‡å»¶è¿Ÿã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase0-title",
   "metadata": {},
   "source": [
    "## ç¬¬é›¶é˜¶æ®µï¼šåŸºç¡€ä¸è®¾ç½®\n",
    "\n",
    "æˆ‘ä»¬å°†å®‰è£…åº“å¹¶è®¾ç½®è¿è¡Œç¯å¢ƒã€‚æˆ‘ä»¬éœ€è¦ `langchain-tavily` æ¥ä¸ºç ”ç©¶ä»£ç†æä¾›æœç´¢å·¥å…·ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install-libs",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q -U langchain-nebius langchain langgraph rich python-dotenv langchain-tavily"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "import-and-keys",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment variables loaded and tracing is set up.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Pydantic for data modeling\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# LangChain components\n",
    "from langchain_nebius import ChatNebius\n",
    "from langchain_tavily import TavilySearch\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# LangGraph components\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "# For pretty printing\n",
    "from rich.console import Console\n",
    "from rich.markdown import Markdown\n",
    "\n",
    "# --- API Key and Tracing Setup ---\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Agentic Architecture - Meta-Controller (Nebius)\"\n",
    "\n",
    "required_vars = [\"NEBIUS_API_KEY\", \"LANGCHAIN_API_KEY\", \"TAVILY_API_KEY\"]\n",
    "for var in required_vars:\n",
    "    if var not in os.environ:\n",
    "        print(f\"Warning: Environment variable {var} not set.\")\n",
    "\n",
    "print(\"Environment variables loaded and tracing is set up.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase1-title",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºä¸“é•¿ä»£ç†\n",
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å°†åˆ›å»ºæˆ‘ä»¬çš„ä¸“å®¶ä»£ç†å›¢é˜Ÿã€‚æ¯ä¸ªä»£ç†éƒ½æ˜¯ä¸€ä¸ªç®€å•çš„é“¾ï¼Œå…·æœ‰ç‰¹å®šçš„è§’è‰²è®¾å®šï¼Œå¯¹äºç ”ç©¶ä»£ç†ï¼Œè¿˜ä¼šé…å¤‡å·¥å…·ã€‚æˆ‘ä»¬å°†æŠŠå®ƒä»¬å°è£…æˆèŠ‚ç‚¹å‡½æ•°ï¼Œä»¥ä¾¿åœ¨ LangGraph ä¸­ä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "specialist-agents-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Specialist agents defined successfully.\n"
     ]
    }
   ],
   "source": [
    "console = Console()\n",
    "llm = ChatNebius(model=\"mistralai/Mixtral-8x22B-Instruct-v0.1\", temperature=0)\n",
    "search_tool = TavilySearch(max_results=3)\n",
    "\n",
    "# Define the state for the overall graph\n",
    "class MetaAgentState(TypedDict):\n",
    "    user_request: str\n",
    "    next_agent_to_call: Optional[str]\n",
    "    generation: str\n",
    "\n",
    "# A helper factory function to create specialist agent nodes\n",
    "def create_specialist_node(persona: str, tools: list = None):\n",
    "    \"\"\"Factory to create a specialist agent node.\"\"\"\n",
    "    system_prompt = f\"You are a specialist agent with the following persona: {persona}. Respond directly and concisely to the user's request based on your role.\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", system_prompt),\n",
    "        (\"human\", \"{user_request}\")\n",
    "    ])\n",
    "    \n",
    "    if tools:\n",
    "        chain = prompt | llm.bind_tools(tools)\n",
    "    else:\n",
    "        chain = prompt | llm\n",
    "        \n",
    "    def specialist_node(state: MetaAgentState) -> Dict[str, Any]:\n",
    "        result = chain.invoke({\"user_request\": state['user_request']})\n",
    "        return {\"generation\": result.content}\n",
    "    \n",
    "    return specialist_node\n",
    "\n",
    "# 1. Generalist Agent Node\n",
    "generalist_node = create_specialist_node(\n",
    "    \"You are a friendly and helpful generalist AI assistant. You handle casual conversation and simple questions.\"\n",
    ")\n",
    "\n",
    "# 2. Research Agent Node\n",
    "research_agent_node = create_specialist_node(\n",
    "    \"You are an expert researcher. You must use your search tool to find information to answer the user's question.\",\n",
    "    tools=[search_tool]\n",
    ")\n",
    "\n",
    "# 3. Coding Agent Node\n",
    "coding_agent_node = create_specialist_node(\n",
    "    \"You are an expert Python programmer. Your task is to write clean, efficient Python code based on the user's request. Provide only the code, wrapped in markdown code blocks, with minimal explanation.\"\n",
    ")\n",
    "\n",
    "print(\"Specialist agents defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase2-title",
   "metadata": {},
   "source": [
    "## ç¬¬äºŒé˜¶æ®µï¼šæ„å»ºå…ƒæ§åˆ¶å™¨\n",
    "\n",
    "è¿™æ˜¯æˆ‘ä»¬ç³»ç»Ÿçš„â€œå¤§è„‘â€ã€‚å…ƒæ§åˆ¶å™¨æ˜¯ä¸€ä¸ªç”± LLM é©±åŠ¨çš„èŠ‚ç‚¹ï¼Œå®ƒå”¯ä¸€çš„èŒè´£å°±æ˜¯å†³å®šå°†è¯·æ±‚è·¯ç”±åˆ°å“ªä¸ªä¸“é•¿ä»£ç†ã€‚å…¶æç¤ºçš„è´¨é‡å¯¹æ•´ä¸ªç³»ç»Ÿçš„æ€§èƒ½è‡³å…³é‡è¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "meta-controller-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Controller node defined successfully.\n"
     ]
    }
   ],
   "source": [
    "# Pydantic model for the controller's routing decision\n",
    "class ControllerDecision(BaseModel):\n",
    "    next_agent: str = Field(description=\"The name of the specialist agent to call next. Must be one of ['Generalist', 'Researcher', 'Coder'].\")\n",
    "    reasoning: str = Field(description=\"A brief reason for choosing the next agent.\")\n",
    "\n",
    "def meta_controller_node(state: MetaAgentState) -> Dict[str, Any]:\n",
    "    \"\"\"The central controller that decides which specialist to call.\"\"\"\n",
    "    console.print(\"--- ğŸ§  Meta-Controller Analyzing Request ---\")\n",
    "    \n",
    "    # Define the specialists and their descriptions for the controller\n",
    "    specialists = {\n",
    "        \"Generalist\": \"Handles casual conversation, greetings, and simple questions.\",\n",
    "        \"Researcher\": \"Answers questions about recent events, complex topics, or anything requiring up-to-date information from the web.\",\n",
    "        \"Coder\": \"Writes Python code based on a user's specification.\"\n",
    "    }\n",
    "    \n",
    "    specialist_descriptions = \"\\n\".join([f\"- {name}: {desc}\" for name, desc in specialists.items()])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        f\"\"\"You are the meta-controller for a multi-agent AI system. Your job is to analyze the user's request and route it to the most appropriate specialist agent.\n",
    "\n",
    "Here are the available specialists:\n",
    "{specialist_descriptions}\n",
    "\n",
    "Analyze the following user request and choose the best specialist to handle it. Provide your decision in the required format.\n",
    "\n",
    "User Request: \"{{user_request}}\"\"\"\"\n",
    "    )\n",
    "    \n",
    "    controller_llm = llm.with_structured_output(ControllerDecision)\n",
    "    chain = prompt | controller_llm\n",
    "    \n",
    "    decision = chain.invoke({\"user_request\": state['user_request']})\n",
    "    console.print(f\"[yellow]Routing decision:[/yellow] Send to [bold]{decision.next_agent}[/bold]. [italic]Reason: {decision.reasoning}[/italic]\")\n",
    "    \n",
    "    return {\"next_agent_to_call\": decision.next_agent}\n",
    "\n",
    "print(\"Meta-Controller node defined successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase3-title",
   "metadata": {},
   "source": [
    "## ç¬¬ä¸‰é˜¶æ®µï¼šç»„è£…å¹¶è¿è¡Œå›¾\n",
    "\n",
    "ç°åœ¨ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ LangGraph å°†æ‰€æœ‰ç»„ä»¶è¿æ¥èµ·æ¥ã€‚å›¾å°†ä»å…ƒæ§åˆ¶å™¨å¼€å§‹ï¼Œç„¶åæ ¹æ®å…¶å†³ç­–ï¼Œé€šè¿‡æ¡ä»¶è¾¹å°†çŠ¶æ€è·¯ç”±åˆ°æ­£ç¡®çš„ä¸“é•¿ä»£ç†èŠ‚ç‚¹ã€‚ä¸“é•¿ä»£ç†è¿è¡Œå®Œæˆåï¼Œå›¾å°†ç»“æŸã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "graph-assembly-code",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta-Controller agent graph compiled successfully.\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "workflow = StateGraph(MetaAgentState)\n",
    "\n",
    "# Add nodes for the controller and each specialist\n",
    "workflow.add_node(\"meta_controller\", meta_controller_node)\n",
    "workflow.add_node(\"Generalist\", generalist_node)\n",
    "workflow.add_node(\"Researcher\", research_agent_node)\n",
    "workflow.add_node(\"Coder\", coding_agent_node)\n",
    "\n",
    "# Set the entry point\n",
    "workflow.set_entry_point(\"meta_controller\")\n",
    "\n",
    "# Define the conditional routing logic\n",
    "def route_to_specialist(state: MetaAgentState) -> str:\n",
    "    \"\"\"Reads the controller's decision and returns the name of the node to route to.\"\"\"\n",
    "    return state[\"next_agent_to_call\"]\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"meta_controller\",\n",
    "    route_to_specialist,\n",
    "    {\n",
    "        \"Generalist\": \"Generalist\",\n",
    "        \"Researcher\": \"Researcher\",\n",
    "        \"Coder\": \"Coder\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# After any specialist runs, the process ends\n",
    "workflow.add_edge(\"Generalist\", END)\n",
    "workflow.add_edge(\"Researcher\", END)\n",
    "workflow.add_edge(\"Coder\", END)\n",
    "\n",
    "meta_agent = workflow.compile()\n",
    "print(\"Meta-Controller agent graph compiled successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "phase4-title",
   "metadata": {},
   "source": [
    "## ç¬¬å››é˜¶æ®µï¼šæ¼”ç¤º\n",
    "\n",
    "è®©æˆ‘ä»¬ä½¿ç”¨å„ç§ä¸åŒçš„æç¤ºæ¥æµ‹è¯•æˆ‘ä»¬çš„å…ƒæ§åˆ¶å™¨ï¼Œçœ‹çœ‹å®ƒæ˜¯å¦èƒ½æ­£ç¡®åœ°å°†å®ƒä»¬åˆ†æ´¾ç»™åˆé€‚çš„ä¸“é•¿ä»£ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "demo-code",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "--- ğŸ’¬ Test 1: General Conversation ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ğŸ§  Meta-Controller Analyzing Request ---\n",
      "Routing decision: Send to Generalist. Reason: The user's request is a simple greeting, which falls under the category of casual conversation handled by the Generalist agent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Final Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello there! How can I help you today?"
      ],
      "text/plain": [
       "Hello there! How can I help you today?"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "--- ğŸ”¬ Test 2: Research Question ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ğŸ§  Meta-Controller Analyzing Request ---\n",
      "Routing decision: Send to Researcher. Reason: The user is asking about a recent event, the latest financial results of a specific company. This requires up-to-date information from the web, which is the specialty of the Researcher agent.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Final Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "NVIDIA's latest financial results, for the quarter ending in April 2024, were exceptionally strong. They reported revenue of $26.04 billion, a significant increase year-over-year, driven largely by their Data Center revenue which hit a record $22.6 billion. Their GAAP earnings per diluted share were $5.98."
      ],
      "text/plain": [
       "NVIDIA's latest financial results, for the quarter ending in April 2024, were exceptionally strong. They reported revenue of $26.04 billion, a significant increase year-over-year, driven largely by their Data Center revenue which hit a record $22.6 billion. Their GAAP earnings per diluted share were $5.98."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "--- ğŸ’» Test 3: Coding Request ---\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ğŸ§  Meta-Controller Analyzing Request ---\n",
      "Routing decision: Send to Coder. Reason: The user is explicitly asking for a Python function, which is a coding task. The Coder agent is the specialist for this.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "Final Response:\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "def fibonacci(n):\n",
       "    \"\"\"Calculates the nth Fibonacci number.\"\"\"\n",
       "    if n <= 0:\n",
       "        return 0\n",
       "    elif n == 1:\n",
       "        return 1\n",
       "    else:\n",
       "        a, b = 0, 1\n",
       "        for _ in range(2, n + 1):\n",
       "            a, b = b, a + b\n",
       "        return b\n",
       "```"
      ],
      "text/plain": [
       "```python\n",
       "def fibonacci(n):\n",
       "    \"\"\"Calculates the nth Fibonacci number.\"\"\"\n",
       "    if n <= 0:\n",
       "        return 0\n",
       "    elif n == 1:\n",
       "        return 1\n",
       "    else:\n",
       "        a, b = 0, 1\n",
       "        for _ in range(2, n + 1):\n",
       "            a, b = b, a + b\n",
       "        return b\n",
       "```"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def run_agent(query: str):\n",
    "    result = meta_agent.invoke({\"user_request\": query})\n",
    "    console.print(\"\\n[bold]Final Response:[/bold]\")\n",
    "    console.print(Markdown(result['generation']))\n",
    "\n",
    "# Test 1: Should be routed to the Generalist\n",
    "console.print(\"--- ğŸ’¬ Test 1: General Conversation ---\")\n",
    "run_agent(\"Hello, how are you today?\")\n",
    "\n",
    "# Test 2: Should be routed to the Researcher\n",
    "console.print(\"\\n--- ğŸ”¬ Test 2: Research Question ---\")\n",
    "run_agent(\"What were NVIDIA's latest financial results?\")\n",
    "\n",
    "# Test 3: Should be routed to the Coder\n",
    "console.print(\"\\n--- ğŸ’» Test 3: Coding Request ---\")\n",
    "run_agent(\"Can you write me a python function to calculate the nth fibonacci number?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conclusion",
   "metadata": {},
   "source": [
    "## ç»“è®º\n",
    "\n",
    "åœ¨æœ¬ç¬”è®°æœ¬ä¸­ï¼Œæˆ‘ä»¬æˆåŠŸå®ç°äº†**å…ƒæ§åˆ¶å™¨**æ¶æ„ã€‚æˆ‘ä»¬çš„æµ‹è¯•æ¸…æ¥šåœ°å±•ç¤ºäº†å…¶æ ¸å¿ƒåŠŸèƒ½ï¼šå……å½“ä¸€ä¸ªæ™ºèƒ½ä¸”åŠ¨æ€çš„è·¯ç”±å™¨ã€‚\n",
    "\n",
    "1.  ç®€å•çš„é—®å€™è¢«æ­£ç¡®è¯†åˆ«å¹¶å‘é€ç»™**é€šç”¨ä»£ç†**ã€‚\n",
    "2.  å…³äºè¿‘æœŸé‡‘èæ–°é—»çš„æŸ¥è¯¢è¢«åˆ†æ´¾ç»™**ç ”ç©¶ä»£ç†**ï¼Œåè€…ä½¿ç”¨å…¶æœç´¢å·¥å…·è·å–äº†æœ€æ–°ä¿¡æ¯ã€‚\n",
    "3.  è¯·æ±‚ä»£ç ç‰‡æ®µçš„éœ€æ±‚è¢«è·¯ç”±åˆ°**ç¼–ç ä»£ç†**ï¼Œåè€…æä¾›äº†æ ¼å¼è‰¯å¥½ä¸”æ­£ç¡®çš„å‡½æ•°ã€‚\n",
    "\n",
    "è¿™ç§æ¨¡å¼åœ¨æ„å»ºå¯æ‰©å±•ä¸”æ˜“ç»´æŠ¤çš„ AI ç³»ç»Ÿæ—¶å…·æœ‰å¼‚å¸¸å¼ºå¤§çš„èƒ½åŠ›ã€‚é€šè¿‡åˆ†ç¦»å…³æ³¨ç‚¹ï¼Œæ¯ä¸ªä¸“é•¿ä»£ç†éƒ½å¯ä»¥ç‹¬ç«‹æ”¹è¿›ï¼Œè€Œä¸ä¼šå½±å“å…¶ä»–ä»£ç†ã€‚ç³»ç»Ÿçš„æ•´ä½“æ™ºèƒ½åªéœ€é€šè¿‡æ·»åŠ æ–°çš„ã€æ›´å¼ºå¤§çš„ä¸“é•¿ä»£ç†å¹¶è®©å…ƒæ§åˆ¶å™¨äº†è§£å®ƒä»¬å³å¯å¾—åˆ°æå‡ã€‚è™½ç„¶æ§åˆ¶å™¨æœ¬èº«å¯èƒ½æˆä¸ºæ½œåœ¨ç“¶é¢ˆï¼Œä½†å®ƒä½œä¸ºçµæ´»ç¼–æ’è€…çš„è§’è‰²ï¼Œæ˜¯é«˜çº§ä»£ç†å¼è®¾è®¡ä¸­çš„åŸºçŸ³ã€‚"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
